---
title: "A tutorial on the cumulative calibration assessment and the cumulcalib package"
output:
  pdf_document: default
  rmarkdown::html_vignette: default
  html_document:
    df_print: paged
vignette: "%\\VignetteIndexEntry{cumulcalib} %\\VignetteEngine{knitr::rmarkdown} %\\VignetteEncoding{UTF-8}\n"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=5, fig.height=3
  ) 
library(knitr)
```


## Introduction

_cumulcalib_ is an R package for the implementation of the cumulative calibration assessment for risk prediction models. 

In order to use _cumulcalib_ efficiently you should be reasonably familiar with the underlying statistical methodologt. please visit the related publication. You can also read the other vignette in this package that covers the basic idea. 

We illustrate the use of this package through a running example.

## Setup: A running example based on GUSTO data
We use data from the GUSTO-I study which is widely used in predictive analytics. These data are available from the _predtools_ package,


```{r}
  library(predtools)
  data(gusto)
  set.seed(1)
```

The outcome is 30-day mortality (we denote it by $y$). We also convert the Killip score to a binary variable (cut off >1):

```{r}
  gusto$y <- gusto$day30
  gusto$kill <- (as.numeric(gusto$Killip)>1)*1
```
  
To create a semi-realistic scenario, we use the non-US sub-sample of the data for model development and the US sub-sample for model validation, and fit a logistic regression model on the development sample.

```{r}
  dev_data <- gusto[!gusto$regl %in% c(1, 7, 9, 10, 11, 12, 14, 15),] #The regl variable containst location codes
  val_data <- gusto[gusto$regl %in% c(1, 7, 9, 10, 11, 12, 14, 15),]
  model <- glm(y ~ age + miloc + pmi + kill + pmin(sysbp,100) + pulse, data=dev_data, family=binomial(link="logit"))
```

The prevalence of the outcome in the development and validation samples are, respectively, `r round(mean(dev_data$y),2)` and `r round(mean(val_data$y),2)`. Here are the coefficients of the model:

```{r echo=FALSE}
  kable(cbind("Coefficients"=summary(model)$coefficients[,1]))
```

We use this model to predict the outcome in the validation sample:
```{r}
  val_data$pi  <- predict(model, type="response", newdata=val_data)
```

Let's start with the conventional calibration plot for this model, which is an estimate of the average value of the true risk at a given level of predicted risk (using the _calibration_plot_ function of the _predtools_ package):

```{r, fig.width = 4, fig.height = 4}
  predtools::calibration_plot(val_data, obs="y", pred="pi")
```
The model seems to be well calibrated. One issue with such a plot is that it generally requires binning (as done above) or smoothing. This is because predicted risks often have many levels, and within each level of predicted risk only a few, and mostly only one, $y$ is observed. This requirement makes this assessment somewhat subjective: whether to use binning or smoothing, if the former, how many bins? If the latter, which smoothing method, and with which parameters?


## Objective assessment of calibration on the cumulative domain
The cumulative calibration assessment is based on the behavior of standardized partial sum of **prediction errors** (i.e., $y-\pi$, after ordering the data ascendingly on $\pi$). The promise is that if the model is calibrated, 
the resulting partial sum, after suitable standardization, will converge to the Brownian motion in the [0,1] interval.

details of such standardization is provided in the original publication. In brief, for the convergence to Brownian motion to work, the 'time' jump at step i should be proportional to the variance of $y_i$, which, under the hypothesis that the model is calibrated, is $\pi_i(1-\pi_i)$. This approach does not require any regulrization.

### Using _cumulcalib_ package for calibration assessment
To assess cumulative calibration using this approach via the _cumujlcalib_ package, we use a simple function call and store the results:

```{r}
  library(cumulcalib)
  res <- cumulcalib(val_data$y, val_data$pi) 
```

Let's explore what type of results this call returns:

```{r}
summary(res)
```

The summary will always report two metrics: $C_n$, which is the sample estimate of mean calibration error ($E(y-\pi)$), and $C^*$ which is a the maximum absolute cumulative prediction error, divided by the number of observations. $C^*$ is a 'distance' metric, similar to metrics such as Emax or the Integrated Calibration Index.

The other components in the summary depend on the Method requested for inference on model calibration. The default method is the Brownian Bridge test, which operates by combining two statistic: $C_n$, and maximum absolute deviation of the random walk from the line that connects its origin and end points. The test statistic related to the former is $S_n$, which is the Z score for mean calibration error with a unit normal distirbution under the null hypothesis, and $B^*$ for the latter, which has a Kolmogorov distribution under the null hypothesis.

Because it can be shown that these two statistics are independent, one combined p-value can be generated using the Fisher's method.

Based on the above results, both mean calibration p-value, and the bridged random walk p-value show that the data are compatible with the model being calibrated in this sample. Indeed, the combined p-value of `r res$pval` also indicated lack of strong evidence against the model being calibrated for the US population.

### Graphical assessment of calibration using _cumulcalib_ package

The returning object from the function call can be directly plotted:

```{r}
  plot(res)
```

The X axis is the 'time' value (running from 0 to 1). The corresponding predicted values are shown on the second X-axis on top. The default Y axis is the standardized sum (cumulative sum divided by the square root of the sum of variance of predicted risks). The second Y axis provides 'scaled' cumulative sum (divided by total observations). This means the terminal value of the random walk can be marked on this axis to identify mean calibration error.

The blue and red vertical lines, respectively, show the terminal value of the random walk and its maximum bridged deviation (given the default method is BB). The dotted lines of the same color mark the critical value of the test (by default at 5% significance level, see the documentation for the cumulcalib() function on how to disable inference lines or change significance level).

Here it is obvious that neither the terminal value (mean calibration error) nor the maximum deviation reach statistical significance. 

## What about a miscalibrated model?
To demonstrate how a miscalibrated model might lool, we repeat the above steps, this time fitting a model based on only a subsample of 500 rows from the development sample. The intuition is that because of the small development sample, the model might not be calibrated.

```{r}
  dev_data2 <- dev_data[sample(nrow(dev_data), 500, replace=F),]
  model2 <- glm(y ~ age + miloc + pmi + kill + pmin(sysbp,100) + pulse, data=dev_data2, family=binomial(link="logit"))
  val_data$pi2  <- predict(model2, type="response", newdata=val_data)
```

Let's look at the calibration plot
```{r}
  predtools::calibration_plot(val_data, obs="y", pred="pi2")
```

The model is indeed not as well calibrated as the model based on the full development sample. Let's repeat the cumulative calibration exercise
  
```{r}
  res2 <- cumulcalib(val_data$y, val_data$pi2)
  summary(res2)
  plot(res2)
```

Here, the miscalibration is quite obvious: the model seems to be making under-estimated risks at low values (the increasing trend of cumulative prediction error), followed by overestimating the risk later (the declining curve). This inverse U shape is a typical signature of an overfitted model.

If we repeat the similar steps for inference, we will obtain the following p-values: `r res2$pval_by_component[1]` for mean miscalibration, `r res2$pval_by_component[2]` for the bridge component, and `r res2$pval` for the unified test, clearly indicating miscalibration.

In this case, it is obvious that the model is mean-calibrated, but the bridge component of the two-part Browniann bridge test rejects the hypothesis that the model is calibrated. 
